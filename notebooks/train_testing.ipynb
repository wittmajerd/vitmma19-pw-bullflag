{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4580344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:40:08) [MSC v.1938 64 bit (AMD64)]\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "GPU count: 1\n",
      "GPU computation successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    \n",
    "    # Quick tensor test\n",
    "    x = torch.randn(1000, 1000).cuda()\n",
    "    y = torch.randn(1000, 1000).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(\"GPU computation successful!\")\n",
    "else:\n",
    "    print(\"No GPU detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd60672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milyen modellel dolgozunk? és hogyan várja az adatokat?\n",
    "# Nem mindehol van volume oszlop mert ez is elég szarul volt specifikálva a feladatkiírásban. Tehát lehet még azt is ki kell dobni ahol van\n",
    "# maradjunk a data with labelsnél majd kicsit átírjuk az adatfeldolgozást ha jól működik ez a pole hozzáadás és kell is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46df785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X: shape=(3285263, 32, 4), dtype=float32\n",
      "train_y: shape=(3285263,), dtype=int64\n",
      "val_X: shape=(702434, 32, 4), dtype=float32\n",
      "val_y: shape=(702434,), dtype=int64\n",
      "test_X: shape=(702513, 32, 4), dtype=float32\n",
      "test_y: shape=(702513,), dtype=int64\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"C:/msc_2/DL/vitmma19-pw-bullflag/data\")\n",
    "PROCESSED_FILE = DATA_DIR / \"processed_data.npz\"\n",
    "\n",
    "\n",
    "def load_processed_dataset(npz_file: Path) -> dict[str, np.ndarray]:\n",
    "    if not npz_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {npz_file}\")\n",
    "    with np.load(npz_file) as data:\n",
    "        return {key: data[key] for key in data.files}\n",
    "\n",
    "\n",
    "dataset = load_processed_dataset(PROCESSED_FILE)\n",
    "for key, array in dataset.items():\n",
    "    print(f\"{key}: shape={array.shape}, dtype={array.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6bc537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(64, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.global_pool(x)  # collapse spatial dims\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def save_model(self, filepath: str):\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath: str):\n",
    "        self.load_state_dict(torch.load(filepath))\n",
    "        self.eval()\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def save_model(self, filepath: str):\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "\n",
    "    def load_model(self, filepath: str):\n",
    "        self.load_state_dict(torch.load(filepath))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464b3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalCNN(nn.Module):\n",
    "    \"\"\"1D CNN with residual blocks for sequence windows shaped (B, T, F).\"\"\"\n",
    "    def __init__(self, input_channels: int, num_classes: int, channels: tuple[int, ...] = (64, 128, 256)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = input_channels\n",
    "        for out_ch in channels:\n",
    "            layers.append(nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1))\n",
    "            layers.append(nn.BatchNorm1d(out_ch))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            in_ch = out_ch\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_ch, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.transpose(1, 2)  # (B, F, T)\n",
    "        x = self.backbone(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class BiGRUClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional GRU using final hidden state.\"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, num_classes: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _, h_n = self.gru(x)\n",
    "        h_last = torch.cat((h_n[-2], h_n[-1]), dim=1)  # concat directions\n",
    "        return self.fc(h_last)\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"Lightweight Transformer encoder pooling CLS token.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        num_classes: int,\n",
    "        d_model: int = 128,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b = x.size(0)\n",
    "        cls = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat([cls, self.input_proj(x)], dim=1)\n",
    "        enc = self.encoder(x)\n",
    "        return self.head(enc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f0d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/10], Loss: 0.2218\n",
      "Epoch [2/10], Loss: 0.2146\n",
      "Epoch [3/10], Loss: 0.2129\n",
      "Epoch [4/10], Loss: 0.2113\n",
      "Epoch [5/10], Loss: 0.2102\n",
      "Epoch [6/10], Loss: 0.2091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     65\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\wittd\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wittd\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wittd\\anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"C:/msc_2/DL/vitmma19-pw-bullflag/data\")\n",
    "PROCESSED_FILE = DATA_DIR / \"processed_data.npz\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "data = np.load(DATA_DIR / \"processed_data.npz\")\n",
    "train_data = data['train_X']\n",
    "train_labels = data['train_y']\n",
    "\n",
    "# Create DataLoader\n",
    "# train_dataset = TensorDataset(\n",
    "#     torch.tensor(train_data, dtype=torch.float32).unsqueeze(1),\n",
    "#     torch.tensor(train_labels, dtype=torch.long),\n",
    "# )\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_data, dtype=torch.float32),\n",
    "    torch.tensor(train_labels, dtype=torch.long),\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "# model = CNNClassifier(num_classes=len(np.unique(train_labels))).to(device)\n",
    "model = LSTMClassifier(input_size=train_data.shape[2], hidden_size=32, num_layers=1, num_classes=len(np.unique(train_labels))).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, config['num_epochs'], device)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2f122",
   "metadata": {},
   "source": [
    "CNN\n",
    "Using device: cuda\n",
    "Epoch [1/10], Loss: 0.2217\n",
    "Epoch [2/10], Loss: 0.2147\n",
    "Epoch [3/10], Loss: 0.2117\n",
    "Epoch [4/10], Loss: 0.2102\n",
    "Epoch [5/10], Loss: 0.2092\n",
    "Epoch [6/10], Loss: 0.2089\n",
    "Epoch [7/10], Loss: 0.2085\n",
    "Epoch [8/10], Loss: 0.2083\n",
    "Epoch [9/10], Loss: 0.2083\n",
    "Epoch [10/10], Loss: 0.2082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3adb9f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3285263, 1, 32, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1d9ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([32, 1, 32, 4])\n",
      "Label batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_loader:\n",
    "    print(\"Input batch shape:\", inputs.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbadd393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.2357 train_acc=0.955 val_loss=0.5747 val_acc=0.883\n",
      "Epoch 2: train_loss=0.2288 train_acc=0.955 val_loss=0.4579 val_acc=0.895\n",
      "Epoch 3: train_loss=0.2260 train_acc=0.955 val_loss=0.4188 val_acc=0.942\n",
      "Epoch 4: train_loss=0.2242 train_acc=0.955 val_loss=0.4586 val_acc=0.942\n",
      "Epoch 5: train_loss=0.2227 train_acc=0.955 val_loss=0.4446 val_acc=0.888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m best_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 71\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n\u001b[0;32m     73\u001b[0m         val_loss, val_acc \u001b[38;5;241m=\u001b[39m run_epoch(model, val_loader, criterion, \u001b[38;5;28;01mNone\u001b[39;00m, device)\n",
      "Cell \u001b[1;32mIn[13], line 47\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     45\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 47\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m yb)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     49\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_REGISTRY = {\n",
    "    \"temporal_cnn\": lambda cfg, n_feats, n_classes: TemporalCNN(n_feats, n_classes),\n",
    "    \"bigru\": lambda cfg, n_feats, n_classes: BiGRUClassifier(\n",
    "        input_size=n_feats,\n",
    "        hidden_size=cfg.get(\"hidden_size\", 128),\n",
    "        num_layers=cfg.get(\"num_layers\", 2),\n",
    "        num_classes=n_classes,\n",
    "        dropout=cfg.get(\"dropout\", 0.2),\n",
    "    ),\n",
    "    \"transformer\": lambda cfg, n_feats, n_classes: TransformerClassifier(\n",
    "        input_size=n_feats,\n",
    "        num_classes=n_classes,\n",
    "        d_model=cfg.get(\"d_model\", 128),\n",
    "        nhead=cfg.get(\"nhead\", 4),\n",
    "        num_layers=cfg.get(\"num_layers\", 2),\n",
    "        dim_feedforward=cfg.get(\"dim_feedforward\", 256),\n",
    "        dropout=cfg.get(\"dropout\", 0.1),\n",
    "    ),\n",
    "}\n",
    "\n",
    "def load_processed_dataset(npz_file: Path) -> dict[str, np.ndarray]:\n",
    "    if not npz_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {npz_file}\")\n",
    "    with np.load(npz_file) as data:\n",
    "        return {key: data[key] for key in data.files}\n",
    "\n",
    "def build_loader(x: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    dataset = TensorDataset(\n",
    "        torch.from_numpy(x).float(),\n",
    "        torch.from_numpy(y).long(),\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None, device=\"cpu\"):\n",
    "    training = optimizer is not None\n",
    "    model.train(training)\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_correct += (preds.argmax(1) == yb).sum().item()\n",
    "        total_samples += xb.size(0)\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "arrays = load_processed_dataset(PROCESSED_FILE)\n",
    "n_features = arrays[\"train_X\"].shape[-1]\n",
    "classes = np.unique(np.concatenate([arrays[\"train_y\"], arrays.get(\"val_y\", []), arrays.get(\"test_y\", [])]))\n",
    "num_classes = len(classes)\n",
    "\n",
    "model_builder = MODEL_REGISTRY[config.get(\"model_name\", \"temporal_cnn\")]\n",
    "model = model_builder(config, n_features, num_classes).to(device)\n",
    "\n",
    "train_loader = build_loader(arrays[\"train_X\"], arrays[\"train_y\"], config[\"batch_size\"], shuffle=True)\n",
    "val_loader = build_loader(arrays[\"val_X\"], arrays[\"val_y\"], config[\"batch_size\"], shuffle=False) if \"val_X\" in arrays else None\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    if val_loader:\n",
    "        val_loss, val_acc = run_epoch(model, val_loader, criterion, None, device)\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.3f} \"\n",
    "                f\"val_loss={val_loss:.4f} val_acc={val_acc:.3f}\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32b59b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 01 | train_loss=0.2355 train_acc=0.955 train_f1=0.140 | val_loss=0.6307 val_acc=0.883 val_f1=0.135\n",
      "Epoch 02 | train_loss=0.2286 train_acc=0.955 train_f1=0.140 | val_loss=0.4519 val_acc=0.933 val_f1=0.139\n",
      "Epoch 03 | train_loss=0.2262 train_acc=0.955 train_f1=0.141 | val_loss=0.5165 val_acc=0.925 val_f1=0.137\n",
      "Epoch 04 | train_loss=0.2243 train_acc=0.955 train_f1=0.142 | val_loss=0.6051 val_acc=0.884 val_f1=0.135\n",
      "Epoch 05 | train_loss=0.2227 train_acc=0.955 train_f1=0.142 | val_loss=0.4628 val_acc=0.898 val_f1=0.136\n",
      "Epoch 06 | train_loss=0.2217 train_acc=0.955 train_f1=0.142 | val_loss=0.4065 val_acc=0.942 val_f1=0.139\n",
      "Epoch 07 | train_loss=0.2208 train_acc=0.955 train_f1=0.143 | val_loss=0.4043 val_acc=0.942 val_f1=0.140\n",
      "Epoch 08 | train_loss=0.2202 train_acc=0.955 train_f1=0.143 | val_loss=0.4947 val_acc=0.901 val_f1=0.137\n",
      "Epoch 09 | train_loss=0.2196 train_acc=0.955 train_f1=0.143 | val_loss=0.4367 val_acc=0.941 val_f1=0.139\n",
      "Epoch 10 | train_loss=0.2189 train_acc=0.955 train_f1=0.144 | val_loss=0.5339 val_acc=0.890 val_f1=0.142\n",
      "Test  | loss=0.6898 acc=0.890 f1=0.135\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"temporal_cnn\": lambda cfg, n_feats, n_classes: TemporalCNN(n_feats, n_classes),\n",
    "    \"bigru\": lambda cfg, n_feats, n_classes: BiGRUClassifier(\n",
    "        input_size=n_feats,\n",
    "        hidden_size=cfg.get(\"hidden_size\", 128),\n",
    "        num_layers=cfg.get(\"num_layers\", 2),\n",
    "        num_classes=n_classes,\n",
    "        dropout=cfg.get(\"dropout\", 0.2),\n",
    "    ),\n",
    "    \"transformer\": lambda cfg, n_feats, n_classes: TransformerClassifier(\n",
    "        input_size=n_feats,\n",
    "        num_classes=n_classes,\n",
    "        d_model=cfg.get(\"d_model\", 128),\n",
    "        nhead=cfg.get(\"nhead\", 4),\n",
    "        num_layers=cfg.get(\"num_layers\", 2),\n",
    "        dim_feedforward=cfg.get(\"dim_feedforward\", 256),\n",
    "        dropout=cfg.get(\"dropout\", 0.1),\n",
    "    ),\n",
    "}\n",
    "\n",
    "def load_processed_dataset(npz_file: Path) -> dict[str, np.ndarray]:\n",
    "    if not npz_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {npz_file}\")\n",
    "    with np.load(npz_file) as data:\n",
    "        return {key: data[key] for key in data.files}\n",
    "\n",
    "def build_loader(x: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    dataset = TensorDataset(torch.from_numpy(x).float(), torch.from_numpy(y).long())\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def prepare_dataloaders(arrays: dict[str, np.ndarray], batch_size: int) -> dict[str, DataLoader]:\n",
    "    loaders = {}\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        x_key, y_key = f\"{split}_X\", f\"{split}_y\"\n",
    "        if x_key in arrays and y_key in arrays and len(arrays[x_key]) > 0:\n",
    "            loaders[split] = build_loader(arrays[x_key], arrays[y_key], batch_size, shuffle=(split == \"train\"))\n",
    "    return loaders\n",
    "\n",
    "def run_phase(model, loader, criterion, device, optimizer=None):\n",
    "    if loader is None:\n",
    "        return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    training = optimizer is not None\n",
    "    model.train(training)\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    preds_all, targets_all = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        if training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        pred_labels = logits.argmax(1)\n",
    "        total_correct += (pred_labels == yb).sum().item()\n",
    "        total_samples += xb.size(0)\n",
    "        preds_all.append(pred_labels.detach().cpu())\n",
    "        targets_all.append(yb.detach().cpu())\n",
    "    preds_concat = torch.cat(preds_all)\n",
    "    targets_concat = torch.cat(targets_all)\n",
    "    macro_f1 = f1_score(targets_concat, preds_concat, average=\"macro\")\n",
    "    return total_loss / total_samples, total_correct / total_samples, macro_f1\n",
    "\n",
    "def train_and_validate():\n",
    "    config = {\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 10\n",
    "    }\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    arrays = load_processed_dataset(PROCESSED_FILE)\n",
    "    n_features = arrays[\"train_X\"].shape[-1]\n",
    "    label_arrays = [arrays[key] for key in arrays if key.endswith(\"_y\")]\n",
    "    classes = np.unique(np.concatenate(label_arrays))\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    loaders = prepare_dataloaders(arrays, config[\"batch_size\"])\n",
    "\n",
    "    model_builder = MODEL_REGISTRY[config.get(\"model_name\", \"temporal_cnn\")]\n",
    "    model = model_builder(config, n_features, num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    for epoch in range(1, config[\"num_epochs\"] + 1):\n",
    "        train_loss, train_acc, train_f1 = run_phase(model, loaders.get(\"train\"), criterion, device, optimizer)\n",
    "        val_loss, val_acc, val_f1 = run_phase(model, loaders.get(\"val\"), criterion, device)\n",
    "\n",
    "        msg = (f\"Epoch {epoch:02d} | \"\n",
    "               f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} train_f1={train_f1:.3f} | \"\n",
    "               f\"val_loss={val_loss:.4f} val_acc={val_acc:.3f} val_f1={val_f1:.3f}\")\n",
    "        print(msg)\n",
    "\n",
    "        if not np.isnan(val_f1) and val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    if \"test\" in loaders:\n",
    "        test_loss, test_acc, test_f1 = run_phase(model, loaders[\"test\"], criterion, device)\n",
    "        print(f\"Test  | loss={test_loss:.4f} acc={test_acc:.3f} f1={test_f1:.3f}\")\n",
    "\n",
    "\n",
    "train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adat kimentés külön a teszteléshez? milyen eval kell egyáltalán?\n",
    "# adatletöltés scriptbe \n",
    "# https://bmeedu-my.sharepoint.com/:u:/g/personal/gyires-toth_balint_vik_bme_hu/IQAlEFc87da4SLpRVTCs81KwATOAjf5GzI-IxEED_nGrjh0?e=eGgGec&download=1\n",
    "# eval metrikák kódolása\n",
    "# repo rendberakás, pathek, conténerezés"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
